services:
  auth:
    build:
      context: ./auth
      dockerfile: Dockerfile
    env_file: env/auth.env
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
    restart: unless-stopped

  cloudflared:
    command: tunnel --no-autoupdate run
    depends_on:
      - watchtower
    env_file: env/cloudflared.env
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    volumes:
      - ./conf/cloudflare/config:/home/nonroot/.cloudflared

  comfyui:
    depends_on:
      - watchtower
    deploy: &gpu-deploy
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    env_file: env/comfyui.env
    image: yanwk/comfyui-boot:cu124-slim
    ports:
      - 8188:8188
    restart: unless-stopped
    volumes:
      - ./data/ComfyUI:/root/ComfyUI
      - ./conf/comfyui/runner-scripts/download-models.txt:/runner-scripts/download-models.txt

  db:
    depends_on:
      - watchtower
    env_file: env/db.env
    image: pgvector/pgvector:pg15
    restart: unless-stopped
    volumes:
      - ./data/postgres:/var/lib/postgresql/data

  docling:
    env_file: env/docling.env
    deploy: &gpu-deploy
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    image: quay.io/docling-project/docling-serve:latest
    restart: unless-stopped

  edgetts:
    depends_on:
      - watchtower
    deploy: &gpu-deploy
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    env_file: env/edgetts.env
    image: travisvn/openai-edge-tts:latest
    ports:
      - 5050:5050
    restart: unless-stopped

  # kokoro:
  #   deploy: *gpu-deploy
  #   image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
  #   ports:
  #     - 8880:8880

  nginx:
    depends_on:
      - cloudflared
      - watchtower
    image: nginx:latest
    ports:
      - 80:80
    restart: unless-stopped
    volumes:
      - ./conf/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf
      - ./conf/nginx/nginx.conf:/etc/nginx/nginx.conf

  ollama:
    depends_on:
      - watchtower
    deploy: *gpu-deploy
    env_file: env/ollama.env
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    restart: unless-stopped
    volumes:
      - ./data/ollama:/root/.ollama

  openwebui:
    depends_on:
      - auth
      - docling
      - db
      - edgetts
      - nginx
      - ollama
      - searxng
      - watchtower
    deploy: *gpu-deploy
    env_file: env/openwebui.env
    extra_hosts:
      - host.docker.internal:host-gateway
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped
    volumes:
      - ./data/openwebui:/app/backend/data

  redis:
    depends_on:
      - watchtower
    env_file: env/redis.env
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 20s
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      timeout: 3s
    image: redis/redis-stack:latest
    restart: unless-stopped
    volumes:
      - ./data/redis:/data

  searxng:
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    cap_drop:
      - ALL
    env_file: env/searxng.env
    depends_on:
      - redis
      - watchtower
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 10s
      test: curl --fail http://localhost:8080/ || exit 1
      timeout: 3s
    image: searxng/searxng:latest
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    restart: unless-stopped
    volumes:
      - ./conf/searxng/settings.yml:/etc/searxng/settings.yml:rw
      - ./conf/searxng/uwsgi.ini:/etc/searxng/uwsgi.ini:rw

  watchtower:
    command: --cleanup --debug --interval 300
    env_file: env/watchtower.env
    image: containrrr/watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
